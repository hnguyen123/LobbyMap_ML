{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # Download the WordNet corpus\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 10604/14000 [00:05<00:01, 2050.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of text_data: 557275\n",
      "First 5 entries in text_data: ['  1', 'COMMENTS OF THE CLASS OF ’85 REGULATORY RESPONSE GROUP', 'ON THE', 'PROPOSED STANDARDS OF PERFORMANCE FOR NEW, RECONSTRUCTED, AND', 'MODIFIED SOURCES AND EMISSIONS GUIDELINES FOR EXISTING SOURCES:']\n",
      "First 5 entries in years: [2022, 2022, 2022, 2022, 2022]\n",
      "First 5 entries in sectors: ['Electric Utilities', 'Electric Utilities', 'Electric Utilities', 'Electric Utilities', 'Electric Utilities']\n",
      "First 5 entries in regions: ['North America', 'North America', 'North America', 'North America', 'North America']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine data \n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # Progress bar for monitoring\n",
    "\n",
    "# Define your evidence query\n",
    "evidence_query = \"GHG Emission Regulation\"\n",
    "\n",
    "# Define the year range to filter\n",
    "start_year = 2013\n",
    "end_year = 2023\n",
    "\n",
    "# File paths\n",
    "jsonl_file_path = r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\data\\processed\\combined.jsonl'\n",
    "csv_file_path = r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\data\\processed\\company_sector_region.csv'\n",
    "\n",
    "# Load the company sector and region data\n",
    "company_info = pd.read_csv(csv_file_path)\n",
    "company_info_dict = company_info.set_index('company_name').to_dict(orient='index')\n",
    "\n",
    "# Initialize lists to collect text data, years, sectors, and regions\n",
    "text_data = []\n",
    "years = []\n",
    "sectors = []\n",
    "regions = []\n",
    "\n",
    "# Read the JSONL file and extract necessary fields with a progress bar\n",
    "with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in tqdm(file, total=14000):  # Update total based on actual size\n",
    "        entry = json.loads(line)\n",
    "        document_id = entry.get('document_id')\n",
    "        sentences = entry.get('sentences', [])\n",
    "        meta_evidences = entry.get('meta', {}).get('evidences', [])\n",
    "\n",
    "        for evidence in meta_evidences:\n",
    "            if isinstance(evidence, list):\n",
    "                for sub_evidence in evidence:\n",
    "                    company_name = sub_evidence.get('company_name')\n",
    "                    evidence_year = sub_evidence.get('evidence_year')\n",
    "                    evidence_query_field = sub_evidence.get('evidence_query')\n",
    "\n",
    "                    # Filter by the target evidence query and year range\n",
    "                    if evidence_query_field == evidence_query and start_year <= evidence_year <= end_year:\n",
    "                        for sentence in sentences:\n",
    "                            text = sentence['text']\n",
    "                            if isinstance(text, str) and text.strip():\n",
    "                                text_data.append(text)\n",
    "                                years.append(evidence_year)\n",
    "\n",
    "                                if company_name in company_info_dict:\n",
    "                                    sector = company_info_dict[company_name]['sector']\n",
    "                                    region = company_info_dict[company_name]['region']\n",
    "                                else:\n",
    "                                    sector = \"Unknown\"\n",
    "                                    region = \"Unknown\"\n",
    "\n",
    "                                sectors.append(sector)\n",
    "                                regions.append(region)\n",
    "            elif isinstance(evidence, dict):\n",
    "                company_name = evidence.get('company_name')\n",
    "                evidence_year = evidence.get('evidence_year')\n",
    "                evidence_query_field = evidence.get('evidence_query')\n",
    "\n",
    "                # Filter by the target evidence query and year range\n",
    "                if evidence_query_field == evidence_query and start_year <= evidence_year <= end_year:\n",
    "                    for sentence in sentences:\n",
    "                        text = sentence['text']\n",
    "                        if isinstance(text, str) and text.strip():\n",
    "                            text_data.append(text)\n",
    "                            years.append(evidence_year)\n",
    "\n",
    "                            if company_name in company_info_dict:\n",
    "                                sector = company_info_dict[company_name]['sector']\n",
    "                                region = company_info_dict[company_name]['region']\n",
    "                            else:\n",
    "                                sector = \"Unknown\"\n",
    "                                region = \"Unknown\"\n",
    "\n",
    "                            sectors.append(sector)\n",
    "                            regions.append(region)\n",
    "\n",
    "# Ensure all elements are strings and remove any that are not\n",
    "text_data = [str(text) for text in text_data if isinstance(text, str) and text.strip()]\n",
    "\n",
    "# Check if all lists have the same length\n",
    "assert len(text_data) == len(years) == len(sectors) == len(regions), \"Mismatch between text data, years, sectors, and regions\"\n",
    "\n",
    "# Print the dimensions and first 5 entries of text_data for debugging\n",
    "print(f\"Dimensions of text_data: {len(text_data)}\")\n",
    "print(\"First 5 entries in text_data:\", text_data[:5])\n",
    "print(\"First 5 entries in years:\", years[:5])\n",
    "print(\"First 5 entries in sectors:\", sectors[:5])\n",
    "print(\"First 5 entries in regions:\", regions[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in the dataset: [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
      "Unique sectors in the dataset: ['Airlines', 'Automobiles', 'Chemicals', 'Consumer Goods & Services', 'Diversified Mining', 'Electric Utilities', 'Oil & Gas', 'Oil & Gas Distribution', 'Other transportation', 'Steel', 'Unknown', 'cement', 'other industrials', 'paper', 'shipping']\n",
      "Unique years in the dataset: ['Africa', 'Asia', 'Australasia', 'Europe', 'Middle East', 'North America', 'South America', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "# Check the unique years/sectors/regions in the dataset\n",
    "unique_years = sorted(set(years))\n",
    "print(f\"Unique years in the dataset: {unique_years}\")\n",
    "\n",
    "unique_sector = sorted(set(sectors))\n",
    "print(f\"Unique sectors in the dataset: {unique_sector}\")\n",
    "\n",
    "unique_region = sorted(set(regions))\n",
    "print(f\"Unique years in the dataset: {unique_region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed NER data found at C:/Users/hoath/Git/LobbyMap_ML/Embeddings/ner_GHG.pkl. Skipping re-computation.\n",
      "Number of documents after cleaning: 318839\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing without region, year, and sector information\n",
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from stopwordsiso import stopwords  # Import stopwords-iso\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define languages for which you want to include stopwords (those with at least 10 documents)\n",
    "languages = [\n",
    "    'af', 'ca', 'cy', 'da', 'de', 'en', 'et', 'es', 'fi', 'fr', 'hr',\n",
    "    'hu', 'id', 'it', 'nl', 'no', 'ro', 'pt', 'pl', 'tl', 'vi', 'sv',\n",
    "    'sl', 'so', 'sw', 'sq', 'lt', 'tr', 'sk'\n",
    "]\n",
    "\n",
    "# Create a combined stopwords list from the selected languages\n",
    "multilingual_stopwords = set()\n",
    "for lang in languages:\n",
    "    multilingual_stopwords.update(stopwords(lang))\n",
    "\n",
    "# Define additional stopwords based on your analysis\n",
    "additional_stopwords = {\n",
    "    \"14\", \"50\", \"70\", \"142\", \"report\", \"combined\", \"statements\", \"corporate\",\n",
    "    \"financial\", \"results\", \"services\", \"emissions\", \"climate\", \"action\",\n",
    "    \"agreement\", \"protection\", \"____\", \"activities\", \"individual\", \"units\", \"source\", \n",
    "    \"vehicle\", \"22\", \"23\", \"26\", \"2022\", \"30\", \"40\", \"25\", \"33\", \"12\", \"13\", \"15\", \n",
    "    \"U.S.C.\", \"Reg.\", \"_\", \"__\", \"page\", \"additional\", \"dow\", \"edf\", \"developpement\", \n",
    "    \"bmw\", \"reg\", \"fig\", \"data\", \"figure\", \"introduction\", \"company\", \"policy\", \n",
    "    \"login\", \"file\", \"user\", \"download\", \"read\", \"january\", \"february\", \"march\", \n",
    "    \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \n",
    "    \"december\", \"fy2017\", \"fy2018\", \"fy2019\", \"fy2020\", \"contact\", \"phone\", \"fax\", \n",
    "    \"email\", \"address\", \"http\", \"https\", \"www\", \"website\", \"association\", \"trade\", \n",
    "    \"management\", \"european\", \"europe\", \"gas\", \"content\", \"search\", \"chapter\", \"mixed\", \n",
    "    \"segment\", \"image\", \"vision\", \"bser\", \"fpls\", \"fpl\", \"caap\", \"statute\", \"guidance\", \n",
    "    \"chief\", \"officer\", \"stakeholder\", \"sincerely\", \"manager\", \"associate\", \"ceo\", \n",
    "    \"executive\", \"chairman\", \"director\", \"president\", \"and\", \"for\", \"the\", \"you\", \n",
    "    \"section\", \"compliance\", \"docket\", \"fy2021\", \"fy2022\", \"fy2023\", \"fyXX\", \"telephone\", \n",
    "    \"call\", \"reach\", \"connect\", \"line\", \"extension\", \"web\", \"url\", \"site\", \"link\", \"click\", \n",
    "    \"net\", \"com\", \"org\", \"gov\", \"domain\", \"facebook\", \"linkedin\", \"twitter\", \"instagram\", \n",
    "    \"social\", \"media\", \"blog\", \"subscribe\", \"pdf\", \"epa\", \"industrial\", \"act\", \"regulation\", \n",
    "    \"comment\", \"public\", \"administrator\", \"judgment\", \"category\", \"regulatory\", \"health\", \n",
    "    \"air\", \"pollution\", \"cause\", \"significantly\", \"endanger\", \"due\", \"potential\"\n",
    "}\n",
    "\n",
    "# Add the additional stopwords to the multilingual stopwords set\n",
    "multilingual_stopwords.update(additional_stopwords)\n",
    "\n",
    "# Convert the multilingual stopwords set back to a list for use in preprocessing\n",
    "multilingual_stopwords = list(multilingual_stopwords)\n",
    "\n",
    "# Function to precompute named entities and store them\n",
    "def precompute_named_entities(text_data, output_file=\"ner_precomputed.pkl\"):\n",
    "    # Check if the precomputed file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Precomputed NER data found at {output_file}. Skipping re-computation.\")\n",
    "        return load_precomputed_named_entities(output_file)\n",
    "    \n",
    "    print(f\"Precomputing NER and saving to {output_file}\")\n",
    "    docs = list(nlp.pipe(text_data, batch_size=50))\n",
    "    named_entities = []\n",
    "\n",
    "    # Extract and remove PERSON and GPE entities from the documents\n",
    "    for doc in docs:\n",
    "        cleaned_text = ' '.join([token.text for token in doc if token.ent_type_ not in ['PERSON', 'GPE']])\n",
    "        named_entities.append(cleaned_text)\n",
    "\n",
    "    # Save the precomputed named entities to a file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(named_entities, f)\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "# Function to load precomputed named entities\n",
    "def load_precomputed_named_entities(file=\"ner_precomputed.pkl\"):\n",
    "    with open(file, 'rb') as f:\n",
    "        precomputed_ner_data = pickle.load(f)\n",
    "    return precomputed_ner_data\n",
    "\n",
    "# Preprocessing function with NER precomputation integration\n",
    "def preprocess_text(text):\n",
    "    # Check if text is not None or empty\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Normalize text (remove accents)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "   # Step 1: Remove legal references and section numbers (handling variations of references)\n",
    "    text = re.sub(r'§*\\s*\\d+[\\(\\d*a-zA-Z\\)]*[\\w]*[\\d\\(\\)]*', '', text)  # Handles cases like \"§ 7687(d)(9)\" and similar section references\n",
    "    text = re.sub(r'\\bsection\\s*\\d+[a-zA-Z\\(\\)]*\\b', '', text, flags=re.IGNORECASE)  # Handles \"Section 111(b)\" and similar\n",
    "    text = re.sub(r'\\barticle\\s*\\d+[a-zA-Z\\(\\)]*\\b', '', text, flags=re.IGNORECASE)  # Handles \"Article 9(b)\" and similar\n",
    "    text = re.sub(r'\\b\\d+u\\.s\\.c\\.\\b', '', text)  # Handles U.S.C. sections (e.g., \"§ 706(2)\")\n",
    "    text = re.sub(r'\\b\\d+\\s*(cfr|usc|u\\.s\\.c\\.|u\\.s\\.)\\b', '', text)  # Handles CFR/USC references\n",
    "    text = re.sub(r'\\b[a-zA-Z]*\\d+[a-zA-Z]*\\b', '', text)  # Removes alphanumeric tokens (e.g., \"EAP-HQ-2021\")\n",
    "    text = re.sub(r'\\b(?:scf|only|initial)\\b', '', text, flags=re.IGNORECASE)  # Specifically removes \"SCF\", \"only\", and \"initial\"\n",
    "    text = re.sub(r'\\b\\d+[a-zA-Z\\(\\)\\d]+\\b', '', text)  # Handles remaining alphanumeric tokens (e.g., \"706(2)\")\n",
    "\n",
    "\n",
    "    # Step 2: Remove standalone numbers and dates\n",
    "    text = re.sub(r'\\b\\d{1,4}\\b', '', text)  # Removes standalone 1-4 digit numbers (e.g., \"85\", \"2020\", \"21\")\n",
    "    text = re.sub(r'\\b\\d+[a-z]*\\b', '', text)  # Removes alphanumeric tokens (e.g., \"50th\", \"85\")\n",
    "    text = re.sub(r'\\b\\d+[-/]\\d+[-/]\\d+\\b', '', text)  # Removes date formats (e.g., \"12/12/2020\" or \"12-12-2020\")\n",
    "    text = re.sub(r'\\b(?:\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4})\\b', '', text)  # Additional date format handling (e.g., \"12/12/2020\")\n",
    "    text = re.sub(r'\\b\\w*\\d+\\w*\\b', '', text)  # Removes alphanumeric tokens (both letters and numbers, e.g., \"EAP-HQ-2021\")\n",
    "\n",
    "\n",
    "    # Consolidated Preprocessing Code for URLs and Special Characters\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Removes all forms of http/https and www URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Removes any remaining special characters\n",
    "\n",
    "\n",
    "    # Step 4: Remove underscores, commas, and special characters, then replace multiple spaces\n",
    "    text = re.sub(r'[_,\\s]+', ' ', text)  # This handles underscores, commas, and multiple spaces\n",
    " \n",
    "    \n",
    "    # Step 5: Remove any remaining special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all non-alphanumeric characters except spaces\n",
    "    \n",
    "\n",
    "    # Remove month abbreviations (case-insensitive)\n",
    "    months_abbr = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    months_pattern = re.compile(r'\\b(' + '|'.join(months_abbr) + r')\\b', flags=re.IGNORECASE)\n",
    "    text = months_pattern.sub('', text)\n",
    "\n",
    "    # Pattern to match \"fy\" followed by either two or four digits (e.g., fy23 or fy2023)\n",
    "    fy_pattern = re.compile(r'\\bfy\\d{2,4}\\b', flags=re.IGNORECASE)\n",
    "    text = fy_pattern.sub('', text)\n",
    "\n",
    "    # Improved PDF Removal: This will capture patterns with 'pdf', whether standalone or within URLs\n",
    "    text = re.sub(r'\\S+\\.pdf\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(pdf\\w*|print\\w*)\\b', '', text, flags=re.IGNORECASE)  # Removes any word starting with 'pdf' or 'print'\n",
    "\n",
    "    text = re.sub(r'\\b(?:epa|us|gpe|nasa|inc|co)\\b', '', text, flags=re.IGNORECASE)  # Add any specific abbreviations here\n",
    "\n",
    "    text = re.sub(r'\\b(?:epa|policy|section)\\s*\\d+[a-zA-Z\\(\\)]*\\b', '', text)  # Handle text like \"EPA Policy Section 111\"\n",
    "\n",
    "\n",
    "\n",
    "    # Remove multilingual stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in multilingual_stopwords])\n",
    "\n",
    "\n",
    "    # Lemmatize each word in the text\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "    # Remove short words (less than 3 characters) and strip extra spaces\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "\n",
    "    # Additional step: Ensure no multiple spaces after cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    text = text.strip()\n",
    "   \n",
    "    return text if len(text.split()) > 2 else None\n",
    "\n",
    "# Check if NER is precomputed, otherwise precompute and load\n",
    "ner_output_file = 'C:/Users/hoath/Git/LobbyMap_ML/Embeddings/ner_GHG.pkl'\n",
    "\n",
    "precomputed_ner_data = precompute_named_entities(text_data, output_file=ner_output_file)\n",
    "\n",
    "# Apply the rest of the preprocessing after named entities are removed\n",
    "cleaned_data = [preprocess_text(doc) for doc in precomputed_ner_data]\n",
    "\n",
    "# Remove any entries that are empty or consist only of whitespace after preprocessing\n",
    "cleaned_data = [doc for doc in cleaned_data if doc and doc.strip()]\n",
    "\n",
    "# save file: \n",
    "with open(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\cleaned_data.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_data, f)\n",
    "\n",
    "\n",
    "# Print the number of documents before and after cleaning\n",
    "print(f\"Number of documents after cleaning: {len(cleaned_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed NER data found at C:/Users/hoath/Git/LobbyMap_ML/Embeddings/ner_GHG.pkl. Skipping re-computation.\n",
      "Number of documents after cleaning: 318839\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing with region, sector and year information\n",
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from stopwordsiso import stopwords  # Import stopwords-iso\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define languages for which you want to include stopwords (those with at least 10 documents)\n",
    "languages = [\n",
    "    'af', 'ca', 'cy', 'da', 'de', 'en', 'et', 'es', 'fi', 'fr', 'hr',\n",
    "    'hu', 'id', 'it', 'nl', 'no', 'ro', 'pt', 'pl', 'tl', 'vi', 'sv',\n",
    "    'sl', 'so', 'sw', 'sq', 'lt', 'tr', 'sk'\n",
    "]\n",
    "\n",
    "# Create a combined stopwords list from the selected languages\n",
    "multilingual_stopwords = set()\n",
    "for lang in languages:\n",
    "    multilingual_stopwords.update(stopwords(lang))\n",
    "\n",
    "# Define additional stopwords based on your analysis\n",
    "additional_stopwords = {\n",
    "    \"14\", \"50\", \"70\", \"142\", \"report\", \"combined\", \"statements\", \"corporate\",\n",
    "    \"financial\", \"results\", \"services\", \"emissions\", \"climate\", \"action\",\n",
    "    \"agreement\", \"protection\", \"____\", \"activities\", \"individual\", \"units\", \"source\", \n",
    "    \"vehicle\", \"22\", \"23\", \"26\", \"2022\", \"30\", \"40\", \"25\", \"33\", \"12\", \"13\", \"15\", \n",
    "    \"U.S.C.\", \"Reg.\", \"_\", \"__\", \"page\", \"additional\", \"dow\", \"edf\", \"developpement\", \n",
    "    \"bmw\", \"reg\", \"fig\", \"data\", \"figure\", \"introduction\", \"company\", \"policy\", \n",
    "    \"login\", \"file\", \"user\", \"download\", \"read\", \"january\", \"february\", \"march\", \n",
    "    \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \n",
    "    \"december\", \"fy2017\", \"fy2018\", \"fy2019\", \"fy2020\", \"contact\", \"phone\", \"fax\", \n",
    "    \"email\", \"address\", \"http\", \"https\", \"www\", \"website\", \"association\", \"trade\", \n",
    "    \"management\", \"european\", \"europe\", \"gas\", \"content\", \"search\", \"chapter\", \"mixed\", \n",
    "    \"segment\", \"image\", \"vision\", \"bser\", \"fpls\", \"fpl\", \"caap\", \"statute\", \"guidance\", \n",
    "    \"chief\", \"officer\", \"stakeholder\", \"sincerely\", \"manager\", \"associate\", \"ceo\", \n",
    "    \"executive\", \"chairman\", \"director\", \"president\", \"and\", \"for\", \"the\", \"you\", \n",
    "    \"section\", \"compliance\", \"docket\", \"fy2021\", \"fy2022\", \"fy2023\", \"fyXX\", \"telephone\", \n",
    "    \"call\", \"reach\", \"connect\", \"line\", \"extension\", \"web\", \"url\", \"site\", \"link\", \"click\", \n",
    "    \"net\", \"com\", \"org\", \"gov\", \"domain\", \"facebook\", \"linkedin\", \"twitter\", \"instagram\", \n",
    "    \"social\", \"media\", \"blog\", \"subscribe\", \"pdf\", \"epa\", \"industrial\", \"act\", \"regulation\", \n",
    "    \"comment\", \"public\", \"administrator\", \"judgment\", \"category\", \"regulatory\", \"health\", \n",
    "    \"air\", \"pollution\", \"cause\", \"significantly\", \"endanger\", \"due\", \"potential\"\n",
    "}\n",
    "\n",
    "# Add the additional stopwords to the multilingual stopwords set\n",
    "multilingual_stopwords.update(additional_stopwords)\n",
    "\n",
    "# Convert the multilingual stopwords set back to a list for use in preprocessing\n",
    "multilingual_stopwords = list(multilingual_stopwords)\n",
    "\n",
    "# Function to precompute named entities and store them\n",
    "def precompute_named_entities(text_data, output_file=\"ner_precomputed.pkl\"):\n",
    "    # Check if the precomputed file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Precomputed NER data found at {output_file}. Skipping re-computation.\")\n",
    "        return load_precomputed_named_entities(output_file)\n",
    "    \n",
    "    print(f\"Precomputing NER and saving to {output_file}\")\n",
    "    docs = list(nlp.pipe(text_data, batch_size=50))\n",
    "    named_entities = []\n",
    "\n",
    "    # Extract and remove PERSON and GPE entities from the documents\n",
    "    for doc in docs:\n",
    "        cleaned_text = ' '.join([token.text for token in doc if token.ent_type_ not in ['PERSON', 'GPE']])\n",
    "        named_entities.append(cleaned_text)\n",
    "\n",
    "    # Save the precomputed named entities to a file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(named_entities, f)\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "# Function to load precomputed named entities\n",
    "def load_precomputed_named_entities(file=\"ner_precomputed.pkl\"):\n",
    "    with open(file, 'rb') as f:\n",
    "        precomputed_ner_data = pickle.load(f)\n",
    "    return precomputed_ner_data\n",
    "\n",
    "# Preprocessing function with NER precomputation integration\n",
    "def preprocess_text(text):\n",
    "    # Check if text is not None or empty\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Normalize text (remove accents)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "   # Step 1: Remove legal references and section numbers (handling variations of references)\n",
    "    text = re.sub(r'§*\\s*\\d+[\\(\\d*a-zA-Z\\)]*[\\w]*[\\d\\(\\)]*', '', text)  # Handles cases like \"§ 7687(d)(9)\" and similar section references\n",
    "    text = re.sub(r'\\bsection\\s*\\d+[a-zA-Z\\(\\)]*\\b', '', text, flags=re.IGNORECASE)  # Handles \"Section 111(b)\" and similar\n",
    "    text = re.sub(r'\\barticle\\s*\\d+[a-zA-Z\\(\\)]*\\b', '', text, flags=re.IGNORECASE)  # Handles \"Article 9(b)\" and similar\n",
    "    text = re.sub(r'\\b\\d+u\\.s\\.c\\.\\b', '', text)  # Handles U.S.C. sections (e.g., \"§ 706(2)\")\n",
    "    text = re.sub(r'\\b\\d+\\s*(cfr|usc|u\\.s\\.c\\.|u\\.s\\.)\\b', '', text)  # Handles CFR/USC references\n",
    "    text = re.sub(r'\\b[a-zA-Z]*\\d+[a-zA-Z]*\\b', '', text)  # Removes alphanumeric tokens (e.g., \"EAP-HQ-2021\")\n",
    "    text = re.sub(r'\\b(?:scf|only|initial)\\b', '', text, flags=re.IGNORECASE)  # Specifically removes \"SCF\", \"only\", and \"initial\"\n",
    "    text = re.sub(r'\\b\\d+[a-zA-Z\\(\\)\\d]+\\b', '', text)  # Handles remaining alphanumeric tokens (e.g., \"706(2)\")\n",
    "\n",
    "\n",
    "    # Step 2: Remove standalone numbers and dates\n",
    "    text = re.sub(r'\\b\\d{1,4}\\b', '', text)  # Removes standalone 1-4 digit numbers (e.g., \"85\", \"2020\", \"21\")\n",
    "    text = re.sub(r'\\b\\d+[a-z]*\\b', '', text)  # Removes alphanumeric tokens (e.g., \"50th\", \"85\")\n",
    "    text = re.sub(r'\\b\\d+[-/]\\d+[-/]\\d+\\b', '', text)  # Removes date formats (e.g., \"12/12/2020\" or \"12-12-2020\")\n",
    "    text = re.sub(r'\\b(?:\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{2,4})\\b', '', text)  # Additional date format handling (e.g., \"12/12/2020\")\n",
    "    text = re.sub(r'\\b\\w*\\d+\\w*\\b', '', text)  # Removes alphanumeric tokens (both letters and numbers, e.g., \"EAP-HQ-2021\")\n",
    "\n",
    "\n",
    "    # Consolidated Preprocessing Code for URLs and Special Characters\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Removes all forms of http/https and www URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Removes any remaining special characters\n",
    "\n",
    "\n",
    "    # Step 4: Remove underscores, commas, and special characters, then replace multiple spaces\n",
    "    text = re.sub(r'[_,\\s]+', ' ', text)  # This handles underscores, commas, and multiple spaces\n",
    " \n",
    "    \n",
    "    # Step 5: Remove any remaining special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all non-alphanumeric characters except spaces\n",
    "    \n",
    "\n",
    "    # Remove month abbreviations (case-insensitive)\n",
    "    months_abbr = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    months_pattern = re.compile(r'\\b(' + '|'.join(months_abbr) + r')\\b', flags=re.IGNORECASE)\n",
    "    text = months_pattern.sub('', text)\n",
    "\n",
    "    # Pattern to match \"fy\" followed by either two or four digits (e.g., fy23 or fy2023)\n",
    "    fy_pattern = re.compile(r'\\bfy\\d{2,4}\\b', flags=re.IGNORECASE)\n",
    "    text = fy_pattern.sub('', text)\n",
    "\n",
    "    # Improved PDF Removal: This will capture patterns with 'pdf', whether standalone or within URLs\n",
    "    text = re.sub(r'\\S+\\.pdf\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(pdf\\w*|print\\w*)\\b', '', text, flags=re.IGNORECASE)  # Removes any word starting with 'pdf' or 'print'\n",
    "\n",
    "    text = re.sub(r'\\b(?:epa|us|gpe|nasa|inc|co)\\b', '', text, flags=re.IGNORECASE)  # Add any specific abbreviations here\n",
    "\n",
    "    text = re.sub(r'\\b(?:epa|policy|section)\\s*\\d+[a-zA-Z\\(\\)]*\\b', '', text)  # Handle text like \"EPA Policy Section 111\"\n",
    "\n",
    "\n",
    "\n",
    "    # Remove multilingual stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in multilingual_stopwords])\n",
    "\n",
    "\n",
    "    # Lemmatize each word in the text\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "    # Remove short words (less than 3 characters) and strip extra spaces\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
    "\n",
    "    # Additional step: Ensure no multiple spaces after cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    text = text.strip()\n",
    "   \n",
    "    return text if len(text.split()) > 2 else None\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a list to store cleaned data along with metadata\n",
    "cleaned_data_with_metadata = []\n",
    "\n",
    "# Ensure NER is precomputed, otherwise precompute and load\n",
    "ner_output_file = 'C:/Users/hoath/Git/LobbyMap_ML/Embeddings/ner_GHG.pkl'\n",
    "precomputed_ner_data = precompute_named_entities(text_data, output_file=ner_output_file)\n",
    "\n",
    "\n",
    "# Apply the rest of the preprocessing after named entities are removed\n",
    "for i, doc in enumerate(precomputed_ner_data):\n",
    "    cleaned_text = preprocess_text(doc)  # Clean the text\n",
    "    if cleaned_text and cleaned_text.strip():\n",
    "        # Append a dictionary that includes the cleaned text along with its metadata\n",
    "        cleaned_data_with_metadata.append({\n",
    "            'text': cleaned_text,\n",
    "            'year': years[i],\n",
    "            'region': regions[i],\n",
    "            'sector': sectors[i]\n",
    "        })\n",
    "\n",
    "# Save cleaned data with metadata to a file\n",
    "with open(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\cleaned_data_with_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_data_with_metadata, f)\n",
    "\n",
    "# Print the number of documents before and after cleaning\n",
    "print(f\"Number of documents after cleaning: {len(cleaned_data_with_metadata)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: comment class response\n",
      "Year: 2022\n",
      "Region: North America\n",
      "Sector: Electric Utilities\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned data with metadata\n",
    "with open(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\cleaned_data_with_metadata.pkl', 'rb') as f:\n",
    "    cleaned_data_with_metadata = pickle.load(f)\n",
    "\n",
    "# Example: Access the first entry's cleaned text, year, region, and sector\n",
    "first_entry = cleaned_data_with_metadata[0]\n",
    "print(f\"Cleaned Text: {first_entry['text']}\")\n",
    "print(f\"Year: {first_entry['year']}\")\n",
    "print(f\"Region: {first_entry['region']}\")\n",
    "print(f\"Sector: {first_entry['sector']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of text_data: 318839\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dimensions of text_data: {len(cleaned_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in text_data before preprocessing: 557275\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents in text_data before preprocessing: {len(text_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment class response group', 'proposed standard performance new reconstructed', 'modified source guideline existing source', 'oil natural sector review', 'united state environmental agency']\n"
     ]
    }
   ],
   "source": [
    "# Print a few examples of the preprocessed text\n",
    "print(cleaned_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample before preprocessing: ['  1', 'COMMENTS OF THE CLASS OF ’85 REGULATORY RESPONSE GROUP', 'ON THE', 'PROPOSED STANDARDS OF PERFORMANCE FOR NEW, RECONSTRUCTED, AND', 'MODIFIED SOURCES AND EMISSIONS GUIDELINES FOR EXISTING SOURCES:']\n",
      "Sample after preprocessing: [None, 'comment class response group', None, 'proposed standard performance new reconstructed', 'modified source guideline existing source']\n"
     ]
    }
   ],
   "source": [
    "# Check some examples of text before preprocessing\n",
    "print(\"Sample before preprocessing:\", text_data[:5])\n",
    "\n",
    "# Apply the preprocessing step\n",
    "preprocessed_text_data = [preprocess_text(doc) for doc in text_data]\n",
    "\n",
    "# Check some examples after preprocessing\n",
    "print(\"Sample after preprocessing:\", preprocessed_text_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty or too short documents removed: 58948\n"
     ]
    }
   ],
   "source": [
    "# Identify and remove empty or very short documents\n",
    "cleaned_data = [doc for doc in text_data if len(doc.strip()) > 3]\n",
    "\n",
    "# Check if any documents were too short or empty\n",
    "empty_docs = len(text_data) - len(cleaned_data)\n",
    "print(f\"Number of empty or too short documents removed: {empty_docs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to check if specific patterns are present\n",
    "def check_for_patterns(text_data):\n",
    "    url_pattern = re.compile(r'(http[s]?://\\S+|www\\.\\S+)')\n",
    "    month_abbr = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    pdf_pattern = re.compile(r'\\b(pdf\\w*|print\\w*)\\b', re.IGNORECASE)\n",
    "    \n",
    "    for i, doc in enumerate(text_data):\n",
    "        if url_pattern.search(doc) or any(month in doc for month in month_abbr) or pdf_pattern.search(doc):\n",
    "            print(f\"Potential issues found in document {i}:\")\n",
    "            print(doc)\n",
    "\n",
    "# Run the check on your preprocessed text data\n",
    "check_for_patterns(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader length: 19928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 19928/19928 [32:50<00:00, 10.12batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Embedding \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Define a custom dataset class for SentenceTransformer\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Explicitly set the device to CPU\n",
    "device = \"cpu\"\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "batch_size = 16  # Reduce batch size to avoid memory issues\n",
    "num_workers = 0  # Number of CPU cores to use for data loading\n",
    "\n",
    "\n",
    "dataset = TextDataset(cleaned_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Debugging: Ensure that DataLoader is working properly\n",
    "print(\"DataLoader length:\", len(dataloader))\n",
    "\n",
    "# Generate embeddings in batches with a progress bar\n",
    "embeddings = []\n",
    "for batch in tqdm(dataloader, desc=\"Generating Embeddings\", unit=\"batch\"):\n",
    "    # Ensure batch is processed on the CPU\n",
    "    batch_embeddings = embedding_model.encode(batch, convert_to_tensor=True, device=device)\n",
    "    embeddings.append(batch_embeddings)\n",
    "\n",
    "    # Debugging: Print after each batch to ensure loop is running\n",
    "    #print(f\"Processed batch size: {len(batch_embeddings)}\")\n",
    "\n",
    "# Concatenate all the embeddings into one tensor\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Convert back to NumPy array if needed\n",
    "embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "# Step 2: Save the embeddings to a .npy file\n",
    "np.save(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\embeddings_GHG_clean.npy', embeddings)\n",
    "print(\"Embeddings saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (318839, 384)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\embeddings_GHG_clean.npy')\n",
    "\n",
    "# Check the shape of the embeddings\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 2 topics...\n",
      "Silhouette score for 2 topics: -0.002788221463561058\n",
      "Testing 5 topics...\n",
      "Silhouette score for 5 topics: -0.008580033667385578\n",
      "Testing 10 topics...\n",
      "Silhouette score for 10 topics: -0.008722503669559956\n",
      "Testing 15 topics...\n",
      "Silhouette score for 15 topics: -0.014485998079180717\n",
      "Testing 20 topics...\n",
      "Silhouette score for 20 topics: -0.018786627799272537\n",
      "Best number of topics: 2 with silhouette score: -0.002788221463561058\n"
     ]
    }
   ],
   "source": [
    "# Initial search for optimal number of topics\n",
    "# Necessary Imports\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Define UMAP and HDBSCAN models for topic modeling\n",
    "umap_model = UMAP(random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_samples=5, min_cluster_size=10)\n",
    "\n",
    "# Load data: \n",
    "with open(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\cleaned_data.pkl', 'rb') as f:\n",
    "    cleaned_data = pickle.load(f)\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = np.load(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\embeddings_GHG_clean.npy')\n",
    "\n",
    "\n",
    "# Function to find the best number of topics using silhouette score\n",
    "def find_best_num_topics(cleaned_data, precomputed_embeddings, topic_numbers=[2, 5, 10, 15, 20]):\n",
    "    best_score = -1\n",
    "    best_num_topics = None\n",
    "    best_model = None\n",
    "\n",
    "    # Loop over the specified topic numbers\n",
    "    for num_topics in topic_numbers:\n",
    "        print(f\"Testing {num_topics} topics...\")\n",
    "\n",
    "        # Create BERTopic model with the specified number of topics\n",
    "        topic_model = BERTopic(nr_topics=num_topics, embedding_model=None, umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "\n",
    "        # Fit the model on the text (cleaned_data) and embeddings (precomputed_embeddings)\n",
    "        topics, probabilities = topic_model.fit_transform(cleaned_data, precomputed_embeddings)\n",
    "\n",
    "        # Calculate the silhouette score\n",
    "        if len(set(topics)) > 1:  # Silhouette score requires at least 2 clusters\n",
    "            score = silhouette_score(precomputed_embeddings, topics)\n",
    "            print(f\"Silhouette score for {num_topics} topics: {score}\")\n",
    "\n",
    "            # Update the best model if this one is better\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_num_topics = num_topics\n",
    "                best_model = topic_model\n",
    "\n",
    "    print(f\"Best number of topics: {best_num_topics} with silhouette score: {best_score}\")\n",
    "    return best_model, best_num_topics\n",
    "\n",
    "# Example usage\n",
    "# cleaned_data: Preprocessed text data (your text input)\n",
    "# precomputed_embeddings: Embeddings generated for cleaned_data\n",
    "\n",
    "# Find the best number of topics for 2, 5, 10, 15, and 20 topics\n",
    "best_model, best_num_topics = find_best_num_topics(cleaned_data, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 2 topics...\n",
      "Silhouette score for 2 topics: -0.0005553557421080768\n",
      "Testing 3 topics...\n",
      "Silhouette score for 3 topics: -0.00076147576328367\n",
      "Testing 4 topics...\n",
      "Silhouette score for 4 topics: -0.002105161314830184\n",
      "\n",
      "Best model based on Silhouette Score: 2 with silhouette score: -0.0005553557421080768\n"
     ]
    }
   ],
   "source": [
    "# Further search for optimal number of topics\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance, KeyBERTInspired\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the precomputed embeddings and cleaned data\n",
    "with open(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\cleaned_data.pkl', 'rb') as f:\n",
    "    cleaned_data = pickle.load(f)\n",
    "\n",
    "embeddings = np.load(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\embeddings_GHG_clean.npy')\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Define UMAP and HDBSCAN models for dimensionality reduction and clustering\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Define the ClassTfidfTransformer with both recommended parameters\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
    "\n",
    "# Define the Maximal Marginal Relevance (MMR) model for topic representation\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Define the KeyBERTInspired model for further refinement\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# Chain the MMR and KeyBERTInspired models together\n",
    "representation_model = [mmr_model, keybert_model]\n",
    "\n",
    "\n",
    "# Function to find the best number of topics using silhouette score and topic coherence\n",
    "def find_best_num_topics(cleaned_data, precomputed_embeddings, topic_numbers=[2, 3, 4]):\n",
    "    best_silhouette_score = -1\n",
    "    best_num_topics = None\n",
    "    best_model_silhouette = None\n",
    "\n",
    "\n",
    "    # Loop over the specified topic numbers\n",
    "    for num_topics in topic_numbers:\n",
    "        print(f\"Testing {num_topics} topics...\")\n",
    "\n",
    "        # Initialize BERTopic with custom settings\n",
    "        topic_model = BERTopic(nr_topics=num_topics,\n",
    "                               embedding_model=embedding_model,\n",
    "                               umap_model=umap_model,\n",
    "                               hdbscan_model=hdbscan_model,\n",
    "                               ctfidf_model=ctfidf_model,\n",
    "                               representation_model=representation_model)\n",
    "        \n",
    "\n",
    "        # Fit the model on the text (cleaned_data) and embeddings (precomputed_embeddings)\n",
    "        topics, probs = topic_model.fit_transform(cleaned_data, embeddings=embeddings)\n",
    "\n",
    "        \n",
    "        # Calculate the silhouette score\n",
    "        silhouette = -1  # Initialize to handle errors\n",
    "        if len(set(topics)) > 1:  # Silhouette score requires at least 2 clusters\n",
    "            try:\n",
    "                silhouette = silhouette_score(embeddings, topics)\n",
    "                print(f\"Silhouette score for {num_topics} topics: {silhouette}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error calculating silhouette score for {num_topics} topics: {e}\")\n",
    "            \n",
    "            # Update the best model for silhouette score\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_num_topics = num_topics\n",
    "                best_model_silhouette = topic_model\n",
    "\n",
    "\n",
    "    # Print final best results\n",
    "    print(f\"\\nBest model based on Silhouette Score: {best_num_topics} with silhouette score: {best_silhouette_score}\")\n",
    "   \n",
    "\n",
    "    return best_model_silhouette\n",
    "\n",
    "# Search for best number of topics based on silhouette score and topic coherence\n",
    "best_model_silhouette = find_best_num_topics(cleaned_data, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MMR with diversity=0.3...\n",
      "Silhouette score for MMR diversity=0.3: -0.0008365029934793711\n",
      "Testing MMR with diversity=0.5...\n",
      "Silhouette score for MMR diversity=0.5: -0.0006165122613310814\n",
      "Testing MMR with diversity=0.7...\n",
      "Silhouette score for MMR diversity=0.7: -0.0008051433251239359\n",
      "\n",
      "Best model based on Silhouette Score: MMR diversity=0.5 with silhouette score: -0.0006165122613310814\n"
     ]
    }
   ],
   "source": [
    "#  Search for MMR with fixed number of topics (2)\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance, KeyBERTInspired\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the precomputed embeddings and cleaned data\n",
    "with open(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\cleaned_data.pkl', 'rb') as f:\n",
    "    cleaned_data = pickle.load(f)\n",
    "\n",
    "embeddings = np.load(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\embeddings_GHG_clean.npy')\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Define UMAP and HDBSCAN models for dimensionality reduction and clustering\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Define the ClassTfidfTransformer with both recommended parameters\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
    "\n",
    "# Define the KeyBERTInspired model for further refinement\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "\n",
    "# Function to tune MMR for fixed number of topics (2)\n",
    "def tune_mmr(cleaned_data, precomputed_embeddings, mmr_values=[0.3, 0.5, 0.7]):\n",
    "    best_silhouette_score = -1\n",
    "    best_mmr = None\n",
    "    best_model_silhouette = None\n",
    "\n",
    "    # Loop over the specified MMR values\n",
    "    for mmr_value in mmr_values:\n",
    "        print(f\"Testing MMR with diversity={mmr_value}...\")\n",
    "\n",
    "        # Define the Maximal Marginal Relevance (MMR) model with the current diversity value\n",
    "        mmr_model = MaximalMarginalRelevance(diversity=mmr_value)\n",
    "\n",
    "        # Chain the MMR and KeyBERTInspired models together\n",
    "        representation_model = [mmr_model, keybert_model]\n",
    "\n",
    "        # Initialize BERTopic with fixed number of topics (2) and custom MMR\n",
    "        topic_model = BERTopic(nr_topics=2,\n",
    "                               embedding_model=embedding_model,\n",
    "                               umap_model=umap_model,\n",
    "                               hdbscan_model=hdbscan_model,\n",
    "                               ctfidf_model=ctfidf_model,\n",
    "                               representation_model=representation_model)\n",
    "\n",
    "        # Fit the model on the text (cleaned_data) and embeddings (precomputed_embeddings)\n",
    "        topics, probs = topic_model.fit_transform(cleaned_data, embeddings=embeddings)\n",
    "\n",
    "        # Calculate the silhouette score\n",
    "        silhouette = -1  # Initialize to handle errors\n",
    "        if len(set(topics)) > 1:  # Silhouette score requires at least 2 clusters\n",
    "            try:\n",
    "                silhouette = silhouette_score(embeddings, topics)\n",
    "                print(f\"Silhouette score for MMR diversity={mmr_value}: {silhouette}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error calculating silhouette score for MMR diversity={mmr_value}: {e}\")\n",
    "\n",
    "            # Update the best model for silhouette score\n",
    "            if silhouette > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette\n",
    "                best_mmr = mmr_value\n",
    "                best_model_silhouette = topic_model\n",
    "\n",
    "    # Print final best results\n",
    "    print(f\"\\nBest model based on Silhouette Score: MMR diversity={best_mmr} with silhouette score: {best_silhouette_score}\")\n",
    "\n",
    "    return best_model_silhouette\n",
    "\n",
    "# Tune MMR for best diversity value\n",
    "best_model_silhouette = tune_mmr(cleaned_data, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 16:03:40,945 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic   Count                                       Name  \\\n",
      "0     -1  138121    -1_sector_renewable_utility_sustainable   \n",
      "1      0  180718  0_efficiency_fuel_strategy_sustainability   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [sector, renewable, utility, sustainable, stra...   \n",
      "1  [efficiency, fuel, strategy, sustainability, r...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [reduce ghg avoiding disruptive economic impac...  \n",
      "1  [energy conservation efficiency energy strateg...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoath\\Git\\LobbyMap_ML\\env_python10\\lib\\site-packages\\scipy\\sparse\\_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# FINAL MODEL with fixed number of topics and MMR\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance, KeyBERTInspired\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load data\n",
    "with open(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\cleaned_data.pkl', 'rb') as f:\n",
    "    cleaned_data = pickle.load(f)\n",
    "\n",
    "# Load the precomputed embeddings\n",
    "embeddings = np.load(r'C:\\Users\\hoath\\Git\\LobbyMap_ML\\Embeddings\\embeddings_GHG_clean.npy')\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Define UMAP and HDBSCAN models for dimensionality reduction and clustering\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Define the ClassTfidfTransformer with both recommended parameters\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
    "\n",
    "# Set Maximal Marginal Relevance (MMR) model for topic representation with diversity = 0.5\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "\n",
    "# Define the KeyBERTInspired model for further refinement\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# Chain the MMR and KeyBERTInspired models together\n",
    "representation_model = [mmr_model, keybert_model]\n",
    "\n",
    "# Initialize BERTopic model with UMAP, HDBSCAN, customized ClassTfidfTransformer, and combined representation models\n",
    "topic_model = BERTopic(nr_topics=2,  # Fixed number of topics to 2\n",
    "                       umap_model=umap_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       embedding_model=embedding_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       representation_model=representation_model)\n",
    "\n",
    "# Ensure cleaned_data is a list of strings\n",
    "if not isinstance(cleaned_data, list):\n",
    "    cleaned_data = list(cleaned_data)\n",
    "\n",
    "# Fit the BERTopic model on the precomputed embeddings and text data\n",
    "topics, probs = topic_model.fit_transform(cleaned_data, embeddings=embeddings)\n",
    "\n",
    "# Check if representative documents are being generated\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(20))\n",
    "\n",
    "# Save the updated topic model for future use\n",
    "topic_model.save(r\"C:\\Users\\hoath\\Git\\LobbyMap_ML\\Models\\topic_model_GHG_clean.pkl\",\n",
    "                 save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic   Count                                       Name  \\\n",
      "0     -1  138121    -1_sector_renewable_utility_sustainable   \n",
      "1      0  180718  0_efficiency_fuel_strategy_sustainability   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [sector, renewable, utility, sustainable, stra...   \n",
      "1  [efficiency, fuel, strategy, sustainability, r...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [reduce ghg avoiding disruptive economic impac...  \n",
      "1  [energy conservation efficiency energy strateg...  \n"
     ]
    }
   ],
   "source": [
    "# Check if representative documents are being generated\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
